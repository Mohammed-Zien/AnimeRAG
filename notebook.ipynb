{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb225a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f5d73",
   "metadata": {},
   "source": [
    "# Setups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1741bf73",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c96d7b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d90df",
   "metadata": {},
   "source": [
    "### Cuda Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c46408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device name: NVIDIA GeForce RTX 2060\n",
      "Device memory: 6.44 GB\n",
      "Number of devices: 1\n",
      "Current device: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name()}\")\n",
    "print(f\"Device memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "print(f\"Number of devices: {torch.cuda.device_count()}\")\n",
    "print(f\"Current device: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a174237",
   "metadata": {},
   "source": [
    "### Lang-smith setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dfefd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGSMITH_PROJECT\"]=\"AnimeRAGchain\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"HF_HOME\"]=\"F:/projects/Porfolio/.cash/huggingface\"\n",
    "os.environ[\"HF_TOKEN\"]=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f18bd45",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0949e8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(jsonl_file_path: str) -> list:\n",
    "    \"\"\"Simple custom loader for the enhanced format\"\"\"\n",
    "    from langchain.schema import Document\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    with open(jsonl_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                record = json.loads(line)\n",
    "                \n",
    "                doc = Document(\n",
    "                    page_content=record.get('page_content', ''),\n",
    "                    metadata=record.get('metadata', {})\n",
    "                )\n",
    "                documents.append(doc)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f76f4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4880"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = load_data(\"anime_data.jsonl\")\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "697efaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"F:\\projects\\Porfolio\\.cash\\huggingface\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\c9745ed1d9f207416be6d2e6f8de32d1f16199bf\",\n",
    "    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
    "    cache_folder=os.environ[\"HF_HOME\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de582127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4880 documents and created 5904 chunks\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(f\"Loaded {len(docs)} documents and created {len(splits)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f341e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1608e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f563fb",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f699b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model_id = \"F:\\projects\\Porfolio\\.cash\\huggingface\\models--meta-llama--Meta-Llama-3-8B-Instruct\\snapshots\\\\5f0b02c75b57c5855da9ae460ce51323ea669d8a\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=os.environ[\"HF_HOME\"])\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e95a6f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd569b6e08644f9833429e87c2a8bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,  \n",
    "    low_cpu_mem_usage=True,  \n",
    "    device_map=\"cuda:0\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    cache_dir=os.environ[\"HF_HOME\"],\n",
    "    token=os.environ[\"HF_TOKEN\"]\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcb4d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,  \n",
    "    do_sample=True,     \n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,  \n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "\n",
    "query_construction_pipeline = pipeline(\n",
    "       \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,  \n",
    "    do_sample=False,     \n",
    "    temperature=0.2,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,  \n",
    "    return_full_text=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e3221",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "query_construction_llm = HuggingFacePipeline(pipeline=query_construction_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93201e6e",
   "metadata": {},
   "source": [
    "## prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f2627d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_prompt = ChatPromptTemplate.from_template(\"\"\"You are an anime expert assistant. Use the context below to answer the question accurately. \n",
    "\n",
    "If you can find relevant information in the context, provide a comprehensive answer based on what's available. \n",
    "If no relevant information is found, say \"I don't know.\"\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850ba80a",
   "metadata": {},
   "source": [
    "## Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dbf02d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs, max_chars=5000):\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"title: {doc.metadata.get('title', 'Untitled')}\\n{doc.page_content}\" for doc in docs\n",
    "    )\n",
    "    return context[:max_chars] + \"...\" if len(context) > max_chars else context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2e696a",
   "metadata": {},
   "source": [
    "## RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e4cbdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792985ff",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7cea537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_gpu():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db55f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question, rag_chain):\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"Generating answer...\")\n",
    "    response = rag_chain.invoke(question)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "352ca173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Who is Naruto ? and what are his dreams ?\n",
      "Generating answer...\n",
      "Answer:  \n",
      "Naruto is a ninja from the Hidden Leaf Village. He dreams of becoming the Hokage, the leader of his village. He is also determined to protect his friends and home, even at the expense of his own body. His determination to become Hokage is strong, and he will carry on with the fight for what is important to him, even in the face of danger. \n",
      "\n",
      "I hope this answer is accurate and helpful. Let me know if you have any further questions.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is Naruto ? and what are his dreams ?\"\n",
    "response = ask_question(question, rag_chain)\n",
    "print(f\"Answer: {response}\")\n",
    "cleanup_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dcccf6",
   "metadata": {},
   "source": [
    "### Query Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57789cba",
   "metadata": {},
   "source": [
    "### Multi-Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fb7690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e276fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "333ef051",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mwhat is the main plot of the anime Naruto?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m retrieval_chain = generate_queries | retriever.map() | get_unique_union\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m docs = \u001b[43mretrieval_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mlen\u001b[39m(docs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\projects\\Porfolio\\AnimeRAG\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3034\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3032\u001b[39m                 \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m   3033\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3034\u001b[39m                 \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[32m   3035\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3036\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\projects\\Porfolio\\AnimeRAG\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:387\u001b[39m, in \u001b[36mBaseLLM.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    376\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    378\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     **kwargs: Any,\n\u001b[32m    384\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    385\u001b[39m     config = ensure_config(config)\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    397\u001b[39m         .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    398\u001b[39m         .text\n\u001b[32m    399\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\projects\\Porfolio\\AnimeRAG\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:764\u001b[39m, in \u001b[36mBaseLLM.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    757\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    761\u001b[39m     **kwargs: Any,\n\u001b[32m    762\u001b[39m ) -> LLMResult:\n\u001b[32m    763\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\projects\\Porfolio\\AnimeRAG\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:971\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    957\u001b[39m     run_managers = [\n\u001b[32m    958\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    959\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    969\u001b[39m         )\n\u001b[32m    970\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m    979\u001b[39m     run_managers = [\n\u001b[32m    980\u001b[39m         callback_managers[idx].on_llm_start(\n\u001b[32m    981\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    988\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[32m    989\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\projects\\Porfolio\\AnimeRAG\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:790\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    780\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    781\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    786\u001b[39m     **kwargs: Any,\n\u001b[32m    787\u001b[39m ) -> LLMResult:\n\u001b[32m    788\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    789\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    794\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    797\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    798\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    799\u001b[39m         )\n\u001b[32m    800\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    801\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\projects\\Porfolio\\AnimeRAG\\.venv\\Lib\\site-packages\\langchain_huggingface\\llms\\huggingface_pipeline.py:315\u001b[39m, in \u001b[36mHuggingFacePipeline._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    312\u001b[39m batch_prompts = prompts[i : i + \u001b[38;5;28mself\u001b[39m.batch_size]\n\u001b[32m    314\u001b[39m \u001b[38;5;66;03m# Process batch of prompts\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m responses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpipeline_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;66;03m# Process each response in the batch\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j, response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(responses):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\projects\\Porfolio\\AnimeRAG\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:302\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    300\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    301\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\projects\\Porfolio\\AnimeRAG\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1412\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[32m   1409\u001b[39m     final_iterator = \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   1410\u001b[39m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[32m   1411\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1412\u001b[39m     outputs = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[32m   1414\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\projects\\Porfolio\\AnimeRAG\\.venv\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_item()\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m item = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n\u001b[32m    125\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\projects\\Porfolio\\AnimeRAG\\.venv\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:125\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m    124\u001b[39m item = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m processed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\projects\\Porfolio\\AnimeRAG\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1338\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1336\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1337\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1339\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1340\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\projects\\Porfolio\\AnimeRAG\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:400\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    398\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    403\u001b[39m     generated_sequence = output.sequences\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\projects\\Porfolio\\AnimeRAG\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\projects\\Porfolio\\AnimeRAG\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2597\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2589\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2590\u001b[39m         input_ids=input_ids,\n\u001b[32m   2591\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2592\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2593\u001b[39m         **model_kwargs,\n\u001b[32m   2594\u001b[39m     )\n\u001b[32m   2596\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2597\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2598\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2602\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2604\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2607\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2608\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2609\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2610\u001b[39m         input_ids=input_ids,\n\u001b[32m   2611\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2612\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2613\u001b[39m         **model_kwargs,\n\u001b[32m   2614\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\projects\\Porfolio\\AnimeRAG\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:3548\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3545\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3546\u001b[39m     is_prefill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3548\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n\u001b[32m   3549\u001b[39m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[32m   3550\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   3552\u001b[39m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Retrieve\n",
    "question = \"what is the main plot of the anime Naruto?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3a03397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "multi_query_rag_chain = (\n",
    "    {\"context\": retrieval_chain | format_docs, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04a839fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Summrize the main plot of tha anime one piece.\n",
      "Generating answer...\n",
      "Answer: I don't know. The context provided does not contain information about the anime \"One Piece\". It seems to be a list of anime titles with their scores, synopses, and main characters. There is no information about the plot of \"One Piece\". \n",
      "\n",
      "Note: Please do not add anything to the answer if it's not contained in the context. If you are unsure about the answer, say \"I don't know\" and do not provide any additional information. \n",
      "\n",
      "Please provide a new\n"
     ]
    }
   ],
   "source": [
    "question = \"Summrize the main plot of tha anime one piece.\"\n",
    "\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(\"Generating answer...\")\n",
    "response = multi_query_rag_chain.invoke({\"question\":question})\n",
    "print(f\"Answer: {response}\")\n",
    "cleanup_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd10eed",
   "metadata": {},
   "source": [
    "### RAG-Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32303910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG-Fusion: Related\n",
    "import re\n",
    "template =\"\"\"Generate 4 search queries related to: {question}\n",
    "\n",
    "Return your response as a JSON array of strings:\n",
    "[\"query1\", \"query2\", \"query3\", \"query4\"]\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def clean_simple_queries(text):\n",
    "    \"\"\"Extract queries from simple line format\"\"\"\n",
    "    lines = text.strip().split('\\n')\n",
    "    queries = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith(('Generate', 'Return', 'Format', 'Do not')):\n",
    "            queries.append(line.strip('\"\\''))\n",
    "    \n",
    "    return queries[:4]  # Limit to 4 queries\n",
    "\n",
    "def parse_json_queries(text):\n",
    "    \"\"\"Parse JSON array of queries\"\"\"\n",
    "    import json\n",
    "    try:\n",
    "        # Extract JSON array from the text\n",
    "        json_match = re.search(r'\\[.*\\]', text, re.DOTALL)\n",
    "        if json_match:\n",
    "            return json.loads(json_match.group())\n",
    "        else:\n",
    "            # Fallback to line-based parsing\n",
    "            return clean_simple_queries(text)\n",
    "    except json.JSONDecodeError:\n",
    "        return clean_simple_queries(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f08f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | parse_json_queries\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cbda23c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what is the main plot of naruto anime',\n",
       " 'what is the story of naruto anime',\n",
       " 'what is the summary of naruto anime',\n",
       " 'what is the main storyline of naruto']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is the main plot of the anime Naruto?\"\n",
    "generate_queries.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1964058c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1ecee09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs_RAG_fusion(docs):\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"title: {doc[0].metadata.get('title', 'Untitled')}\\n{doc[0].page_content}\" for doc in docs\n",
    "    )\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "623f525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RAG_Fusion_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion | format_docs_RAG_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47c96bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: who is the main character in the anime cowboy bepop?\n",
      "Generating answer...\n",
      "Answer:  Spike. The main character Spike must choose between life with his newfound family or revenge for his old wounds. The Bebop crew's lives are disrupted by a menace from Spike's past. \n",
      "\n",
      "Please provide a comprehensive answer based on the given context. If no relevant information is found, say \"I don't know.\" \n",
      "\n",
      "Note: The score provided is not relevant to the question. The information provided is about the anime Cowboy Bebop and its related episodes, not the main character. \n",
      "\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "question = \"who is the main character in the anime cowboy bepop?\"\n",
    "\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(\"Generating answer...\")\n",
    "response = RAG_Fusion_rag_chain.invoke({\"question\":question})\n",
    "print(f\"Answer: {response}\")\n",
    "cleanup_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bac045",
   "metadata": {},
   "source": [
    "### Query-Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b570bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "metadata_field_info = [\n",
    "    # Basic identification\n",
    "    AttributeInfo(\n",
    "        name=\"title\",\n",
    "        description=\"The title of the anime\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"type\",\n",
    "        description=\"The type of anime. One of ['TV', 'Movie', 'OVA', 'ONA', 'Special', 'Music']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    \n",
    "    # Genre information\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"Comma-separated list of genres for the anime (e.g., 'Action, Adventure, Drama')\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"genre_primary\",\n",
    "        description=\"The primary/main genre of the anime\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"genre_count\",\n",
    "        description=\"Number of genres associated with the anime\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    \n",
    "    # Ratings and rankings\n",
    "    AttributeInfo(\n",
    "        name=\"score\",\n",
    "        description=\"The anime's rating score (typically 0-10 scale)\",\n",
    "        type=\"float\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rank\",\n",
    "        description=\"The anime's ranking position (lower numbers = higher rank)\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"popularity\",\n",
    "        description=\"The anime's popularity ranking (lower numbers = more popular)\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rank_category\",\n",
    "        description=\"Categorized ranking. One of ['top_50', 'top_100', 'top_500', 'top_1000', 'below_1000']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"popularity_category\",\n",
    "        description=\"Categorized popularity. One of ['very_popular', 'popular', 'moderately_popular', 'niche', 'obscure']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    \n",
    "    # Production information\n",
    "    AttributeInfo(\n",
    "        name=\"studio\",\n",
    "        description=\"The animation studio that produced the anime\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    # Temporal information\n",
    "    AttributeInfo(\n",
    "        name=\"aired_year\",\n",
    "        description=\"The year the anime was first aired\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"decade\",\n",
    "        description=\"The decade when the anime aired (e.g., '2020s', '2010s', '2000s')\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"aired_season\",\n",
    "        description=\"The season when the anime aired. One of ['Spring', 'Summer', 'Fall', 'Winter']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    \n",
    "    # Franchise information\n",
    "    AttributeInfo(\n",
    "        name=\"franchise\",\n",
    "        description=\"The franchise or series the anime belongs to\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"related_count\",\n",
    "        description=\"Number of related entries (sequels, prequels, spin-offs, etc.)\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    \n",
    "    # Boolean flags\n",
    "    AttributeInfo(\n",
    "        name=\"is_movie\",\n",
    "        description=\"Whether the anime is a movie format\",\n",
    "        type=\"boolean\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"is_tv_series\",\n",
    "        description=\"Whether the anime is a TV series format\",\n",
    "        type=\"boolean\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"is_highly_rated\",\n",
    "        description=\"Whether the anime has a high rating (score >= 8.5)\",\n",
    "        type=\"boolean\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"is_popular\",\n",
    "        description=\"Whether the anime is popular (popularity rank <= 1000)\",\n",
    "        type=\"boolean\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"is_major_studio\",\n",
    "        description=\"Whether the anime was produced by a major studio\",\n",
    "        type=\"boolean\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"has_related_entries\",\n",
    "        description=\"Whether the anime has related entries (sequels, prequels, etc.)\",\n",
    "        type=\"boolean\",\n",
    "    ),\n",
    "    \n",
    "    # Content characteristics\n",
    "    AttributeInfo(\n",
    "        name=\"has_long_synopsis\",\n",
    "        description=\"Whether the anime has a detailed synopsis (>500 characters)\",\n",
    "        type=\"boolean\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"synopsis_length_category\",\n",
    "        description=\"Length category of the synopsis. One of ['short', 'medium', 'long']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"Brief synopsis describing the plot, themes, and story of an anime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eb4e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Literal, Optional, Tuple\n",
    "import datetime\n",
    "\n",
    "class AnimeSearch(BaseModel):\n",
    "    \"\"\"Search over a database of anime entries.\"\"\"\n",
    "    \n",
    "    content_search: str = Field(\n",
    "        ...,\n",
    "        description=\"Similarity search query applied to anime synopsis.\",\n",
    "    )\n",
    "    title_search: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Alternate version of the content search query to apply to anime titles. \"\n",
    "            \"Should be succinct and only include key words that could be in an anime \"\n",
    "            \"title.\"\n",
    "        ),\n",
    "    )\n",
    "    min_score: Optional[float] = Field(\n",
    "        None,\n",
    "        description=\"Minimum rating score filter (0-10 scale), inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    max_score: Optional[float] = Field(\n",
    "        None,\n",
    "        description=\"Maximum rating score filter (0-10 scale), exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    earliest_aired_year: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Earliest aired year filter, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    latest_aired_year: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Latest aired year filter, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    min_popularity: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Minimum popularity ranking filter (lower = more popular), inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    max_popularity: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Maximum popularity ranking filter (lower = more popular), exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    type: Optional[Literal['TV', 'Movie', 'OVA', 'ONA', 'Special', 'Music']] = Field(\n",
    "        None,\n",
    "        description=\"Filter for the type of anime. Only use if explicitly specified.\",\n",
    "    )\n",
    "    genre: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"Filter for specific genres (comma-separated). Only use if explicitly specified.\",\n",
    "    )\n",
    "    studio: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"Filter for the animation studio. Only use if explicitly specified.\",\n",
    "    )\n",
    "    is_movie: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"Filter for movie format. Only use if explicitly specified.\",\n",
    "    )\n",
    "    is_tv_series: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"Filter for TV series format. Only use if explicitly specified.\",\n",
    "    )\n",
    "    is_highly_rated: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"Filter for highly rated anime (score >= 8.5). Only use if explicitly specified.\",\n",
    "    )\n",
    "    is_popular: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"Filter for popular anime (popularity rank <= 1000). Only use if explicitly specified.\",\n",
    "    )\n",
    "\n",
    "    def pretty_print(self) -> None:\n",
    "        for field in self.__fields__:\n",
    "            if getattr(self, field) is not None and getattr(self, field) != getattr(\n",
    "                self.__fields__[field], \"default\", None\n",
    "            ):\n",
    "                print(f\"{field}: {getattr(self, field)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5a6d336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =prompt = prompt = \"\"\"\n",
    "You are an expert at generating structured data in JSON format for anime search filters. I want you to output only a JSON object following the AnimeSearch Pydantic model structure below.\n",
    "\n",
    "Here is the model definition:\n",
    "{{\n",
    "    \"content_search\": \"string - Search terms for anime descriptions/summaries/plot content\",\n",
    "    \"title_search\": \"string - Search terms for anime titles/names\",\n",
    "    \"min_score\": float | null - Minimum rating score (0-10 scale),\n",
    "    \"max_score\": float | null - Maximum rating score (0-10 scale),\n",
    "    \"earliest_aired_year\": int | null - Earliest year anime should have aired\",\n",
    "    \"latest_aired_year\": int | null - Latest year anime should have aired\",\n",
    "    \"type\": \"TV\" | \"Movie\" | \"OVA\" | \"ONA\" | \"Special\" | \"Music\" | null - Type of anime format,\n",
    "    \"genre\": \"string | null - Genre names separated by commas if multiple\",\n",
    "    \"studio\": \"string | null - Animation studio name\",\n",
    "}}\n",
    "\n",
    "Instructions:\n",
    "- Strictly follow this JSON format.\n",
    "- Only output the JSON object, no additional explanation.\n",
    "- IMPORTANT: Only fill fields based on what the user explicitly mentions in their query. Do NOT use your internal knowledge about specific anime.\n",
    "- If a field is not specified in the query or irrelevant, set it to null.\n",
    "- If a field is a boolean, set it to true, false, or null as appropriate.\n",
    "- For the type field, only use one of these values: \"TV\", \"Movie\", \"OVA\", \"ONA\", \"Special\", \"Music\", or null.\n",
    "- CRITICAL: You must ALWAYS output the complete JSON object with ALL fields, no matter what. Never truncate or cut off the JSON.\n",
    "- Ensure the JSON object is fully closed with a single closing curly brace.\n",
    "- First output \"START_OF_JSON\" on a line, then the complete JSON object, then \"END_OF_JSON\" on a new line.\n",
    "- The format must be: START_OF_JSON, then JSON object, then END_OF_JSON.\n",
    "\n",
    "Here is an example object:\n",
    "START_OF_JSON\n",
    "{{\n",
    "    \"content_search\": \"\",\n",
    "    \"title_search\": \"\",\n",
    "    \"min_score\": null,\n",
    "    \"max_score\": null,\n",
    "    \"earliest_aired_year\": null,\n",
    "    \"latest_aired_year\": null,\n",
    "    \"type\": null,\n",
    "    \"genre\": null,\n",
    "    \"studio\": null,\n",
    "}}\n",
    "END_OF_JSON\n",
    "\n",
    "Please analyze the user's query and fill in all the JSON object accordingly and make sure to output the full JSON object no matter what.\n",
    "CRITICAL: Always complete the entire JSON with all fields before stopping. Never cut off the JSON mid-field.\n",
    "Output format: START_OF_JSON, then complete JSON object, then END_OF_JSON on a new line.\n",
    "Output in strict JSON syntax, no comments or markdown.\n",
    "userquery: {user_query}\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from langchain_core.pydantic_v1 import ValidationError\n",
    "\n",
    "def query_construction_parser(model_output : str) -> AnimeSearch:\n",
    "    try:\n",
    "        # Look for JSON between START_OF_JSON and END_OF_JSON tags\n",
    "        start_marker = \"START_OF_JSON\"\n",
    "        end_marker = \"END_OF_JSON\"\n",
    "        \n",
    "        start_idx = model_output.find(start_marker)\n",
    "        end_idx = model_output.find(end_marker)\n",
    "        \n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            json_text = model_output[start_idx + len(start_marker):end_idx].strip()\n",
    "        else:\n",
    "            # Fallback to regex if markers not found\n",
    "            match = re.search(r\"\\{.*\\}\", model_output, re.DOTALL)\n",
    "            if not match:\n",
    "                raise ValueError(\"No JSON object found in the model output.\")\n",
    "            json_text = match.group(0).strip()\n",
    "        \n",
    "        data = json.loads(json_text)\n",
    "        if not data[\"content_search\"]:\n",
    "            data[\"content_search\"] = \"\"\n",
    "        if not data[\"title_search\"]:\n",
    "            data[\"title_search\"] = \"\"\n",
    "    except (json.JSONDecodeError, ValidationError) as e:\n",
    "        data = {\n",
    "            \"content_search\": \"\",\n",
    "            \"title_search\": \"\",\n",
    "            \"min_score\": None,\n",
    "            \"max_score\": None,\n",
    "            \"earliest_aired_year\": None,\n",
    "            \"latest_aired_year\": None,\n",
    "            \"type\": None,\n",
    "            \"genre\": None,\n",
    "            \"studio\": None,\n",
    "        }\n",
    "\n",
    "    anime_search = AnimeSearch(**data)\n",
    "    return anime_search\n",
    "\n",
    "def build_chroma_where_filter(anime_search: AnimeSearch) -> dict:\n",
    "    \"\"\"\n",
    "    Converts an AnimeSearch object into a ChromaDB 'where' filter dictionary.\n",
    "    Uses '$and' to combine multiple conditions.\n",
    "    \"\"\"\n",
    "    filters = []\n",
    "\n",
    "    # Score filters\n",
    "    if anime_search.min_score is not None:\n",
    "        filters.append({\"score\": {\"$gte\": anime_search.min_score}})\n",
    "    if anime_search.max_score is not None:\n",
    "        filters.append({\"score\": {\"$lt\": anime_search.max_score}})\n",
    "\n",
    "    # Year filters\n",
    "    if anime_search.earliest_aired_year is not None:\n",
    "        filters.append({\"aired_year\": {\"$gte\": anime_search.earliest_aired_year}})\n",
    "    if anime_search.latest_aired_year is not None:\n",
    "        filters.append({\"aired_year\": {\"$lt\": anime_search.latest_aired_year}})\n",
    "\n",
    "    # Type filter\n",
    "    if anime_search.type is not None:\n",
    "        filters.append({\"type\": anime_search.type})\n",
    "\n",
    "    # Genre filter (assuming genres are stored as a list)\n",
    "    if anime_search.genre is not None:\n",
    "        search_genres = [g.strip().lower() for g in anime_search.genre.split(\",\")]\n",
    "        filters.append({\"genres\": {\"$in\": search_genres}})\n",
    "\n",
    "    # Studio filter\n",
    "    if anime_search.studio is not None:\n",
    "        filters.append({\"studio\": {\"$eq\": anime_search.studio.lower()}})\n",
    "\n",
    "    # Combine all filters with $and\n",
    "    return { \"filters\" : {\"$and\": filters},\n",
    "            \"title_search\": anime_search.title_search.lower() if anime_search.title_search else \"\",\n",
    "            \"content_search\": anime_search.content_search.lower() if anime_search.content_search else \"\"\n",
    "            }\n",
    "\n",
    "def format_docs(filters: dict) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve documents from ChromaDB using content_search and filters.\n",
    "    If title_search is provided, prepend the doc's title.\n",
    "    \"\"\"\n",
    "    query = filters.get(\"content_search\", \"\")\n",
    "    where_filter = filters.get(\"filters\", None)\n",
    "\n",
    "    # Retrieve documents\n",
    "    docs = vectorstore.similarity_search(\n",
    "        query=query,\n",
    "        filter=where_filter,\n",
    "        k=3\n",
    "    ) if where_filter and where_filter.get(\"$and\") else vectorstore.similarity_search(query=query, k=3)\n",
    "\n",
    "    # Build context, always including page_content, optionally adding title\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"title: {doc.metadata.get('title', 'Untitled')}\\n{doc.page_content}\"\n",
    "        for doc in docs\n",
    "    )\n",
    "\n",
    "    return context\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b0b910b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_construction_chain = (\n",
    "   {\"user_query\" : RunnablePassthrough()} | prompt_template | query_construction_llm | query_construction_parser | build_chroma_where_filter | format_docs\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": query_construction_chain ,\"question\": RunnablePassthrough()}\n",
    "    | text_generation_prompt\n",
    "    | text_generation_llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "35324cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "question = \"summrize the main plot of the animes with score higher than 9 and aired after 2023.\"\n",
    "answer = rag_chain.invoke(question)\n",
    "cleanup_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806e9b46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
