{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb225a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f5d73",
   "metadata": {},
   "source": [
    "# Setups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1741bf73",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c96d7b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d90df",
   "metadata": {},
   "source": [
    "### Cuda Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c46408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device name: NVIDIA GeForce RTX 2060\n",
      "Device memory: 6.44 GB\n",
      "Number of devices: 1\n",
      "Current device: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name()}\")\n",
    "print(f\"Device memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "print(f\"Number of devices: {torch.cuda.device_count()}\")\n",
    "print(f\"Current device: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a174237",
   "metadata": {},
   "source": [
    "### Lang-smith setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dfefd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGSMITH_PROJECT\"]=\"AnimeRAGchain\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"HF_HOME\"]=\"F:/projects/Porfolio/.cash/huggingface\"\n",
    "os.environ[\"HF_TOKEN\"]=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f18bd45",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0949e8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(jsonl_file_path: str) -> list:\n",
    "    \"\"\"Simple custom loader for the enhanced format\"\"\"\n",
    "    from langchain.schema import Document\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    with open(jsonl_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                record = json.loads(line)\n",
    "                \n",
    "                doc = Document(\n",
    "                    page_content=record.get('page_content', ''),\n",
    "                    metadata=record.get('metadata', {})\n",
    "                )\n",
    "                documents.append(doc)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2f76f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_data(\"enhanced_anime_data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "82cb86cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4880"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "697efaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
    "    cache_folder=os.environ[\"HF_HOME\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "de582127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4880 documents and created 5904 chunks\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(f\"Loaded {len(docs)} documents and created {len(splits)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e44de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f341e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e1608e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f563fb",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f699b1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516b3672a8fa4cdfa94fe575055ae59d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b47590567f48379f98b9a9d60876fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e35299e13d846cf90ec7b9e6ccb0cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=os.environ[\"HF_HOME\"])\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e95a6f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad48d9e31664e28a02965a1fa6318bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,  \n",
    "    low_cpu_mem_usage=True,  \n",
    "    device_map=\"cuda:0\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    cache_dir=os.environ[\"HF_HOME\"],\n",
    "    token=os.environ[\"HF_TOKEN\"]\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on device: {next(model.parameters()).device}\")\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    return_full_text=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6a3e3221",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93201e6e",
   "metadata": {},
   "source": [
    "## prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f2627d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"You are an anime expert assistant. Use the context below to answer the question accurately. \n",
    "\n",
    "If you can find relevant information in the context, provide a comprehensive answer based on what's available. \n",
    "If no relevant information is found, say \"I don't know.\"\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850ba80a",
   "metadata": {},
   "source": [
    "## Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9dbf02d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs, max_chars=5000):\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"title: {doc.metadata.get('title', 'Untitled')}\\nscore: {doc.metadata.get('score', '')}\\n{doc.page_content}\" for doc in docs\n",
    "    )\n",
    "    return context[:max_chars] + \"...\" if len(context) > max_chars else context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2e696a",
   "metadata": {},
   "source": [
    "## RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1e4cbdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792985ff",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7cea537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_gpu():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "db55f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question, rag_chain):\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"Generating answer...\")\n",
    "    response = rag_chain.invoke(question)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ca173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: what anime have a time travilling machine made of microwave?\n",
      "Generating answer...\n"
     ]
    }
   ],
   "source": [
    "question = \"what anime have a time travilling machine made of microwave?\"\n",
    "response = ask_question(question, rag_chain)\n",
    "print(f\"Answer: {response}\")\n",
    "cleanup_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dcccf6",
   "metadata": {},
   "source": [
    "### Query Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57789cba",
   "metadata": {},
   "source": [
    "### Multi-Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1fb7690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e276fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "333ef051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\AppData\\Local\\Temp\\ipykernel_32884\\170002541.py:10: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique_docs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve\n",
    "question = \"what is the main plot of the anime Naruto?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a03397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "multi_query_rag_chain = (\n",
    "    {\"context\": retrieval_chain | format_docs, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04a839fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Summrize the main plot of tha anime one piece.\n",
      "Generating answer...\n",
      "Answer: I don't know. The context provided does not contain information about the anime \"One Piece\". It seems to be a list of anime titles with their scores, synopses, and main characters. There is no information about the plot of \"One Piece\". \n",
      "\n",
      "Note: Please do not add anything to the answer if it's not contained in the context. If you are unsure about the answer, say \"I don't know\" and do not provide any additional information. \n",
      "\n",
      "Please provide a new\n"
     ]
    }
   ],
   "source": [
    "question = \"Summrize the main plot of tha anime one piece.\"\n",
    "\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(\"Generating answer...\")\n",
    "response = multi_query_rag_chain.invoke({\"question\":question})\n",
    "print(f\"Answer: {response}\")\n",
    "cleanup_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd10eed",
   "metadata": {},
   "source": [
    "### RAG-Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32303910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG-Fusion: Related\n",
    "import re\n",
    "template =\"\"\"Generate 4 search queries related to: {question}\n",
    "\n",
    "Return your response as a JSON array of strings:\n",
    "[\"query1\", \"query2\", \"query3\", \"query4\"]\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def clean_simple_queries(text):\n",
    "    \"\"\"Extract queries from simple line format\"\"\"\n",
    "    lines = text.strip().split('\\n')\n",
    "    queries = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith(('Generate', 'Return', 'Format', 'Do not')):\n",
    "            queries.append(line.strip('\"\\''))\n",
    "    \n",
    "    return queries[:4]  # Limit to 4 queries\n",
    "\n",
    "def parse_json_queries(text):\n",
    "    \"\"\"Parse JSON array of queries\"\"\"\n",
    "    import json\n",
    "    try:\n",
    "        # Extract JSON array from the text\n",
    "        json_match = re.search(r'\\[.*\\]', text, re.DOTALL)\n",
    "        if json_match:\n",
    "            return json.loads(json_match.group())\n",
    "        else:\n",
    "            # Fallback to line-based parsing\n",
    "            return clean_simple_queries(text)\n",
    "    except json.JSONDecodeError:\n",
    "        return clean_simple_queries(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8f08f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | parse_json_queries\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cbda23c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what is the main plot of naruto anime',\n",
       " 'ninja village story naruto',\n",
       " 'naruto anime storyline explained',\n",
       " 'what is the overall plot of naruto']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is the main plot of the anime Naruto?\"\n",
    "generate_queries.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1964058c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1ecee09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs_RAG_fusion(docs, max_chars=5000):\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"title: {doc[0].metadata.get('title', 'Untitled')}\\nscore: {doc[0].metadata.get('score', '')}\\n{doc[0].page_content}\" for doc in docs\n",
    "    )\n",
    "    return context[:max_chars] + \"...\" if len(context) > max_chars else context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "623f525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RAG_Fusion_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion | format_docs_RAG_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "47c96bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Summrize the main story of the anime sword art online\n",
      "Generating answer...\n",
      "Answer:  Sword Art Online is a story about a virtual reality game called Sword Art Online (SAO) where players can experience a new world of gaming. The game's creator, Akihiko Kayaba, traps the players inside the game, and the only way to escape is to clear all one hundred floors. The main characters, Kirito and his friends, must adapt and survive in this new world, all while trying to beat the competition to the top. The story follows their journey as they try\n"
     ]
    }
   ],
   "source": [
    "question = \"Summrize the main story of the anime sword art online\"\n",
    "\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(\"Generating answer...\")\n",
    "response = RAG_Fusion_rag_chain.invoke({\"question\":question})\n",
    "print(f\"Answer: {response}\")\n",
    "cleanup_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b570bf91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
